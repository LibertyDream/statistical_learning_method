{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- [概率分布间的关系](#概率分布间的关系)\n",
    "- [LDA](#LDA)\n",
    "  - [吉布斯抽样法](#吉布斯抽样算法)\n",
    "  - [变分EM推理](#变分EM推理)\n",
    "- [算法实现](#算法实现)\n",
    "\n",
    "# 概率分布间的关系\n",
    "\n",
    "多项分布是二项分布的扩展，针对多元**离散**随机变量$X$，有 $X \\sim \\operatorname{Mult}(n, p)$\n",
    "$$\n",
    "\\begin{aligned} P\\left(X_{1}=n_{1}, X_{2}=n_{2}, \\cdots, X_{k}=n_{k}\\right) &=\\frac{n !}{n_{1} ! n_{2} ! \\cdots n_{k} !} p_{1}^{n_{1}} p_{2}^{n_{2}} \\cdots p_{k}^{n_{k}} \\\\ &=\\frac{n !}{\\prod_{i=1}^{k} n_{i} !} \\prod_{i=1}^{k} p_{i}^{n_{i}} \\end{aligned}\n",
    "$$\n",
    "狄利克雷分布是多元**连续**随机变量的概率分布，记作 $\\theta \\sim \\operatorname{Dir}(\\alpha)$ \n",
    "$$\n",
    "p(\\theta | \\alpha)=\\frac{\\Gamma\\left(\\sum_{i=1}^{k} \\alpha_{i}\\right)}{\\prod_{i=1}^{k} \\Gamma\\left(\\alpha_{i}\\right)} \\prod_{i=1}^{k} \\theta_{i}^{\\alpha_{i}-1}\\\\\n",
    "\\sum_{i=1}^{k} \\theta_{i}=1, \\theta_{i} \\geqslant 0, \\alpha=\\left(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{k}\\right), \\alpha_{i}>0, i=1,2, \\cdots, k_{\\circ}\n",
    "$$\n",
    "$\\Gamma(s)$ 是伽玛函数 $\\Gamma(s)=\\int_{0}^{\\infty} x^{s-1} \\mathrm{e}^{-x} \\mathrm{d} x, \\quad s>0$，满足性质 $\\Gamma(s+1)=s \\Gamma(s)$，当 $s$ 是自然数时，$\\Gamma(s+1) = s!$\n",
    "\n",
    "也可以把狄利克雷分布写成\n",
    "$$\n",
    "p(\\theta | \\alpha)=\\frac{1}{\\mathrm{B}(\\alpha)} \\prod_{i=1}^{k} \\theta_{i}^{\\alpha_{i}-1}\\\\\n",
    "\\mathrm{B}(\\alpha)=\\frac{\\prod_{i=1}^{k} \\Gamma\\left(\\alpha_{i}\\right)}{\\Gamma\\left(\\sum_{i=1}^{k} \\alpha_{i}\\right)}=\\int \\prod_{i=1}^{k} \\theta_{i}^{\\alpha_{i}-1} d \\theta\n",
    "$$\n",
    "$B(\\alpha)$ 是规范化因子，也称多元贝塔函数（贝塔函数的拓展）\n",
    "\n",
    "二项分布是多项分布的特例，贝塔分布是狄利克雷分布的特例。贝塔分布为\n",
    "$$\n",
    "p(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{B(s, t)} x^{s-1}(1-x)^{t-1},} & {0 \\leqslant x \\leqslant 1} \\\\ {0,} & {其他}\\end{array}\\right.\\\\\n",
    "\\mathrm{B}(s, t)=\\frac{\\Gamma(s) \\Gamma(t)}{\\Gamma(s+t)}\n",
    "$$\n",
    "$B(s,t)$ 为贝塔函数\n",
    "\n",
    "如果后验分布和先验分布为同类（相同分布族），则先验分布与后验分布成为**共轭分布**，先验成为共轭先验\n",
    "\n",
    "- 狄利克雷分布属于指数分布族\n",
    "- 狄利克雷分布是多项分布的共轭先验\n",
    "\n",
    "> 指数分布族是指概率分布密度能写成如下形式\n",
    "> $$\n",
    "> p(x | \\eta)=h(x) \\exp \\left\\{\\eta^{\\mathrm{T}} T(x)-A(\\eta)\\right\\}\n",
    "> $$\n",
    "> 多项式分布，泊松分布， gamma 分布，指数分布，beta分布， Dirichlet 分布这些都是一家\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-11_probability_distribution_relation.png)\n",
    "\n",
    "# LDA\n",
    "\n",
    "潜在狄利克雷分配（LDA）是文本集合的生成概率模型，是基于贝叶斯的话题模型。有三要素：**单词集，话题集和文本集**。模型假设\n",
    "\n",
    "- 话题由单词的多项分布表示（单词产生话题）\n",
    "- 文本由话题的多项分布表示（话题产生文本）\n",
    "- 单词分布和话题分布的先验分布都是狄利克雷分布\n",
    "\n",
    "LDA 是概率图模型，板块表示如下。单词分布、话题分布和文本各位置的话题是隐变量，各位置的单词是观测变量\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-09_lda-tikz.png)\n",
    "\n",
    "**文本生成过程**\n",
    "\n",
    "1. 由参数 $\\beta$ 确定的 $Dir(\\beta)$，生成若干单词分布（话题） $\\varphi_{k} \\sim \\operatorname{Dir}(\\beta)，k=1,2,\\ldots,K$\n",
    "2. 由参数 $\\alpha$ 确定的 $Dir(\\alpha)$，生成若干话题分布（文本）$\\theta_{m} \\sim \\operatorname{Dir}(\\alpha)，m=1,2,\\ldots,M$\n",
    "3. 对于文本 $\\mathbf{w}_m$ 里的单词 $w_{mn},n=1,2\\ldots,N$\n",
    "   1. 生成话题 $z_{m n} \\sim \\operatorname{Mult}\\left(\\theta_{m}\\right)$\n",
    "   2. 生成单词 $w_{m n} \\sim \\operatorname{Mult}\\left(\\varphi_{z_{m n}}\\right)$\n",
    "\n",
    "话题个数 $K$ 通常由实验选定。狄利克雷分布的参数 $\\alpha,\\beta$ 也是事先给定。没有先验知识时所有分量默认为 1\n",
    "\n",
    "LDA 模型本身的学习与推理不能直接求解。通常采用吉布斯抽样和变分 EM 算法，前者是蒙特卡罗法（MCMC），通过抽样近似估计，简单但迭代次数多。后者是解析计算进行近似，推理和学习效率高\n",
    "\n",
    "## 吉布斯抽样算法\n",
    "\n",
    "超参数已给定，目标是对联合概率分布 $p(\\mathbf{w},\\mathbf{z},\\theta,\\varphi|\\alpha,\\beta)$ 进行估计，$\\mathbf{w}$ 已知，$\\mathbf{z},\\theta,\\varphi$ 未知。通过积分求和将隐变量 $\\theta$ 和 $\\varphi$ 消掉，得到边缘分布 $p(\\mathbf{w},\\mathbf{z}|\\alpha,\\beta)$。对概率分布 $p(\\mathbf{w}|\\alpha,\\beta,\\mathbf{z})$ 进行吉布斯抽样，得到该分布下的随机样本，利用样本对 $\\mathbf{z},\\theta,\\varphi$ 进行估计。最终得到  $p(\\mathbf{w},\\mathbf{z},\\theta,\\varphi|\\alpha,\\beta)$ 的参数估计。\n",
    "\n",
    "1. 对给定文本的单词序列 $\\mathbf{w}$，每个位置随机指派一个话题，构成整个文本的话题序列 $\\mathbf{z}$\n",
    "\n",
    "2. 循环直到燃烧期\n",
    "\n",
    "   1. 每个位置上计算该处话题的满条件概率分布，进行随机抽样，得到该位置的新话题\n",
    "\n",
    "   $$\n",
    "   p\\left(z_{i} | \\mathbf{z}_{-i}, \\mathbf{w}, \\alpha, \\beta\\right) \\propto \\frac{n_{k v}+\\beta_{v}}{\\sum_{v=1}^{V}\\left(n_{k v}+\\beta_{v}\\right)} \\cdot \\frac{n_{m k}+\\alpha_{k}}{\\sum_{k=1}^{K}\\left(n_{m k}+\\alpha_{k}\\right)}\n",
    "   $$\n",
    "\n",
    "   前一项是由话题生成单词的概率，后一项是由文本生成话题的概率\n",
    "\n",
    "   2. 整体准备两个计数矩阵：话题-单词矩阵 $N_{K \\times V}=\\left[n_{k v}\\right]$ 和文本-话题矩阵 $N_{M \\times K}=[n_{mk}]$。两个矩阵里该位置旧话题计数减 1，计算满条件概率，抽样，得到该处新话题，该处新话题计数加 1。前往下一个位置\n",
    "\n",
    "3. 燃烧期后所有文本的话题序列就是后验概率分布  $p(\\mathbf{z}|\\alpha,\\beta,\\mathbf{w})$ 的样本计数，由此计算模型参数\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\theta_{m k}=& \\frac{n_{m k}+\\alpha_{k}}{\\sum_{k=1}^{K}\\left(n_{m k}+\\alpha_{k}\\right)} \\\\ \\varphi_{k v}=& \\frac{n_{k v}+\\beta_{v}}{\\sum_{v=1}^{V}\\left(n_{k v}+\\beta_{v}\\right)} \\end{aligned}\n",
    "$$\n",
    "\n",
    "## 变分EM推理\n",
    "\n",
    "假设模型是联合概率分布 $p(x,z)$，$x$ 是观测变量（数据），$z$ 是隐变量。目标是学习模型的后验概率分布 $p(z|x)$。考虑用变分分布 $q(z)$ 近似条件概率分布 $p(z|x)$，用 KL 散度计算二者相似性找到与 $p(z|x)$ 在 KL 散度意义下最近的 $q^*(z)$，用该分布近似 $p(z|x)$。\n",
    "\n",
    "> KL 散度是对两个概率分布相似性的一种度量。记作 $D(Q||P)$，对离散随机变量和连续随机变量有\n",
    "> $$\n",
    "> D(Q \\| P)=\\sum_{i} Q(i) \\log \\frac{Q(i)}{P(i)}\\\\\n",
    "> D(Q \\| P)=\\int Q(x) \\log \\frac{Q(x)}{P(x)} \\mathrm{d} x\n",
    "> $$\n",
    "> 由 Jesson 不等式\n",
    "> $$\n",
    "> \\begin{aligned}-D(Q \\| P) &=\\int Q(x) \\log \\frac{P(x)}{Q(x)} \\mathrm{d} x \\\\ & \\leqslant \\log \\int Q(x) \\frac{P(x)}{Q(x)} \\mathrm{d} x \\\\ &=\\log \\int P(x) \\mathrm{d} x=0 \\end{aligned}\n",
    "> $$\n",
    "> 所以 KL 散度具有性质 $D(Q||P) \\geqslant 0$ ，当且仅当两分布相同才为 0。\n",
    ">\n",
    "> KL 散度不对称，也不满足三角不等式，不是传统意义的距离度量\n",
    "\n",
    "对变分分布要求容易处理，所以假设 $q(z)$ 中 $z$ 的所有分量都相互独立，此时变分分布称为平均场。利用 Jensen 不等式，得到 KL 散度最小化可以通过证据下界最大化实现。变分推理由此变成下界最大化问题\n",
    "$$\n",
    "\\begin{aligned} D(q(z) \\| p(z | x)) &=E_{q}[\\log q(z)]-E_{q}[\\log p(z | x)] \\\\ &=E_{q}[\\log q(z)]-E_{q}[\\log p(x, z)]+\\log p(x) \\\\ &=\\log p(x)-\\{E_{q}[\\log p(x, z)]-E_{q}[\\log q(z)]\\}\\end{aligned}\\\\\n",
    "L(q, \\theta)=E_{q}[\\log p(x, z | \\theta)]-E_{q}[\\log q(z)]\n",
    "$$\n",
    "针对LDA模型定义变分分布，应用 EM 变分法。\n",
    "$$\n",
    "q(\\theta, \\mathbf{z} | \\gamma, \\eta)=q(\\theta | \\gamma) \\prod_{n=1}^{N} q\\left(z_{n} | \\eta_{n}\\right)\n",
    "$$\n",
    "一共 N 个词，$\\gamma$ 是狄利克雷分布的参数，$\\eta$ 是多项分布参数\n",
    "\n",
    "目标是对下界 $L(\\gamma, \\eta, \\alpha, \\varphi)$ 最大化，$\\alpha,\\varphi$ 是LDA 模型参数，$\\gamma,\\eta$ 是变分参数。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\gamma, \\eta, \\alpha, \\varphi)&=E_{q}[\\log p(\\theta, \\mathbf{z}, \\mathbf{w} | \\alpha, \\varphi)]-E_{q}[\\log q(\\theta, \\mathbf{z} | \\gamma, \\eta)]\\\\\n",
    " &= E_{q}[\\log p(\\theta | \\alpha)]+E_{q}[\\log p(\\mathbf{z} | \\theta)]+E_{q}[\\log p(\\mathbf{w} | \\mathbf{z}, \\varphi)]-E_{q}[\\log q(\\theta | \\gamma)]-E_{q}[\\log q(\\mathbf{z} | \\eta)] \n",
    " \\end{aligned}\n",
    "$$\n",
    "$\\theta$ 为话题分布，交替 E，M 两步，直到收敛\n",
    "\n",
    "1. E 步：固定参数 $\\alpha,\\varphi$ ，通过变分参数 $\\gamma,\\eta$ 证据下界最大化估计 $\\gamma,\\eta$ \n",
    "2. M步：固定 $\\gamma,\\eta$ ，通过模型参数 $\\alpha,\\varphi$ 证据下界最大化估计 $\\alpha,\\varphi$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法实现\n",
    "\n",
    "**导入相关库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**硬件与版本信息**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "ipywidgets 7.5.0\n",
      "numpy 1.16.4\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -m -v -p ipywidgets,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA 实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    \n",
    "    def __init__(self, T, alpha=None, beta=None):\n",
    "        '''基于吉布斯抽样的潜在狄利克雷分配\n",
    "\n",
    "        Parameters：\n",
    "        -----------------\n",
    "        T:\n",
    "            话题数\n",
    "        alpha：\n",
    "            先验话题狄利克雷分布参数\n",
    "        beta：\n",
    "            先验单词狄利克雷分布参数\n",
    "\n",
    "        Attributes：\n",
    "        --------------------\n",
    "        D:\n",
    "            文本数\n",
    "        N:\n",
    "            单词数\n",
    "        V:\n",
    "            单词标识数，一个标识唯一对应一个单词\n",
    "        phi：\n",
    "            单词分布\n",
    "        theta：\n",
    "            话题分布\n",
    "        '''\n",
    "        self.T = T\n",
    "        \n",
    "        if alpha is None:\n",
    "            self.alpha = (50.0 / self.T) * np.ones(self.T)\n",
    "        else:\n",
    "            self.alpha = alpha * np.ones(self.T)\n",
    "        \n",
    "        if beta is None:\n",
    "            self.beta = 0.01\n",
    "        else:\n",
    "            self.beta = beta\n",
    "            \n",
    "    def __init_params(self, texts, tokens):\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.D = len(texts)\n",
    "        self.V = len(np.unique(self.tokens))\n",
    "        self.N = np.sum(np.array([len(doc) for doc in texts]))\n",
    "        self.word_document = np.zeros(self.N)\n",
    "        \n",
    "        self.beta = self.beta * np.ones(self.V)\n",
    "        \n",
    "        count = 0\n",
    "        for doc_idx, doc in enumerate(texts):\n",
    "            for word_idx, word in enumerate(doc):\n",
    "                word_idx = word_idx + count\n",
    "                self.word_document[word_idx] = doc_idx\n",
    "            count += len(doc)\n",
    "    \n",
    "    def train(self, texts, tokens, burn_limits=2000):\n",
    "        '''在给定语料和标识集上训练 LDA 模型\n",
    "        \n",
    "        Returns\n",
    "        ---------------------\n",
    "        C_wt：\n",
    "            单词-话题计数矩阵\n",
    "        C_dt：\n",
    "            文本-话题计数矩阵\n",
    "        assignments:\n",
    "            抽样时分配给每个单词位置的话题\n",
    "        '''\n",
    "        \n",
    "        self.__init_params(texts, tokens)\n",
    "        C_wt, C_dt, assignments = self.__gibbs_sampler(burn_limits, texts)\n",
    "        self.fit_params(C_wt, C_dt)\n",
    "        return C_wt, C_dt, assignments\n",
    "    \n",
    "    def topic_words_distribution(self, top_n=10):\n",
    "        '''按类别输出各话题下 Top-N 个单词'''\n",
    "        for t in range(self.T):\n",
    "            idx = np.argsort(self.phi[:,t])[::-1][:top_n]\n",
    "            tokens = self.tokens[idx]\n",
    "            for token in tokens:\n",
    "                print('%s\\n' % (str(token)))\n",
    "                \n",
    "    def fit_params(self, C_wt, C_dt):\n",
    "        '''由抽样结果计算参数估计值'''\n",
    "        \n",
    "        self.phi = np.zeros([self.V, self.T])\n",
    "        self.theta = np.zeros([self.D, self.T])\n",
    "        \n",
    "        b,a = self.beta[0], self.alpha[0]\n",
    "        for i in range(self.V):\n",
    "            for j in range(self.T):\n",
    "                self.phi[i,j] = (C_wt[i, j] + b) / (np.sum(C_wt[:,j]) + self.V * b)\n",
    "        \n",
    "        for i in range(self.D):\n",
    "            for j in range(self.T):\n",
    "                self.theta[i,j] = (C_dt[i, j] + a) / (np.sum(C_dt[i,:]) + self.T * a)\n",
    "    \n",
    "        return self.phi, self.theta\n",
    "    \n",
    "    def __full_condition_prob(self, i, d, C_wt, C_dt):\n",
    "        '''计算第 i 个位置的满条件概率'''\n",
    "        \n",
    "        p_vec = np.zeros(self.T)\n",
    "        b, a = self.beta[0], self.alpha[0]\n",
    "        for j in range(self.T):\n",
    "            frac1 = (C_wt[i, j] + b) / (np.sum(C_wt[:, j])+self.V * b)\n",
    "            frac2 = (C_dt[d, j] + a) / (np.sum(C_dt[d, :])+self.T * a)\n",
    "            p_vec[j] = frac1 * frac2\n",
    "        return p_vec / np.sum(p_vec)\n",
    "    \n",
    "    def __gibbs_sampler(self, burn_limits, texts):\n",
    "        '''吉布斯采样估计后验概率'''\n",
    "        \n",
    "        C_wt = np.zeros([self.V, self.T])\n",
    "        C_dt = np.zeros([self.D, self.T])\n",
    "        assignments = np.zeros([self.N, burn_limits + 1])\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            token_idx = np.concatenate(texts)[i]\n",
    "            assignments[i, 0] = np.random.randint(0, self.T)\n",
    "            \n",
    "            doc = self.word_document[i]\n",
    "            C_dt[doc, assignments[i, 0]] += 1\n",
    "            C_wt[token_idx, assignments[i, 0]] += 1\n",
    "            \n",
    "        for i in range(burn_limits):\n",
    "            print('iteration %d of %d' % (i+1, burn_limits))\n",
    "            for j in range(self.N):\n",
    "                token_idx = np.concatenate(texts)[j]\n",
    "                \n",
    "                doc = self.word_document[j]\n",
    "                C_wt[token_idx, assignments[j, i]] -= 1\n",
    "                C_dt[doc, assignments[j, i]] -= 1\n",
    "                \n",
    "                p_topic = self.__full_condition_prob(token_idx, doc, C_wt, C_dt)\n",
    "                sample = np.nonzero(np.random.multinomial(1, p_topic))[0][0]\n",
    "                \n",
    "                C_wt[token_idx, sample] += 1\n",
    "                C_dt[doc, sample] += 1\n",
    "                assignments[j, i+1] = sample\n",
    "            \n",
    "        return C_wt, C_dt, assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**作者：** Daniel Meng\n",
    "\n",
    "**GitHub：**[LibertyDream](https://github.com/LibertyDream)\n",
    "\n",
    "**博客：**[明月轩](https://LIbertydream.github.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
