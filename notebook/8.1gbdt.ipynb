{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度提升\n",
    "\n",
    "回顾一下回归提升树通过前向分步算法求解的过程：\n",
    "$$\n",
    "\\begin{array}{l}{f_{0}(x)=0} \\\\ {f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right), \\quad m=1,2, \\cdots, M} \\\\ {f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right)}\\end{array}\n",
    "$$\n",
    "第 $m$ 步，给定模型 $f(m-1)$ ，求解经验风险极小化\n",
    "$$\n",
    "\\hat{\\Theta}_{m}=\\arg \\min _{\\Theta_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+T\\left(x_{i} ; \\Theta_{m}\\right)\\right)\n",
    "$$\n",
    "以此得到第 $m$ 棵树的参数，而当选用平方损失函数 $L(y, f(x))=(y-f(x))^{2}$ 时，有\n",
    "$$\n",
    "\\begin{aligned} L\\left(y, f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)\\right) &=\\left[y-f_{m-1}(x)-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\\\ &=\\left[r-T\\left(x ; \\Theta_{m}\\right)\\right]^{2} \\end{aligned}\n",
    "$$\n",
    "其中 $r = y- f_{m-1}(x)$，可见回归树只需要拟合残差使得 $r$ 极小即可，很简单也很容易优化，但对于一般损失函数就不太容易了。\n",
    "\n",
    "对此人们提出了梯度提升（gradient boosting)的概念，关键是在于将当前模型损失函数的负梯度值作为回归提升树中残差的近似值，拟合出一棵回归树。\n",
    "\n",
    "## 与梯度下降的比较\n",
    "\n",
    "一般的优化思路是 $L(y,f(x))--f = g(x;w)——L(y,f(x);w)$，由损失和模型的关系到损失和模型参数的关系，所以最后结果是求解参数的负梯度更新参数，这也就是梯度下降的思想。\n",
    "\n",
    "而梯度上升实质上只有前半部分 $L(y,f(x))--L(y,f(x);f)$ ，不再从参数空间中寻找，而是从函数空间中搜索，降低了优化门槛，极大拓展了模型选择空间。当然依旧是依靠负梯度进行更新。\n",
    "\n",
    "## GBDT 算法描述\n",
    "\n",
    "输入：训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$，$x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}, y_{i} \\in \\mathcal{Y} \\subseteq \\mathbf{R}$；损失函数 $L(y,f(x))$\n",
    "\n",
    "输出：回归树 $\\hat{f}(x)$\n",
    "\n",
    "1. 初始化 $f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right)$\n",
    "\n",
    "2. 对 $m=1,2,\\ldots,M$\n",
    "\n",
    "   1. 对 $i = 1,2,\\ldots,N$，计算\n",
    "\n",
    "   $$\n",
    "   r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}\n",
    "   $$\n",
    "\n",
    "   2. 对 $r_{mi}$ 拟合一个回归树，得到第 $m$ 棵树的结点区域 $R_{m j}, j=1,2, \\cdots, J$。\n",
    "   3. 对 $j=1,2, \\cdots, J$，计算 $c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right)$\n",
    "   4. 更新 $f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$\n",
    "\n",
    "3. 得到回归树\n",
    "\n",
    "$$\n",
    "\\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)\n",
    "$$\n",
    "\n",
    "对于2.1步，对于平方损失函数 $L\\left(y, f_{m-1}(x)\\right)=\\frac{1}{2}\\left(y-f_{m-1}(x)\\right)^{2}$，得到的 $r_m$ 就是残差，对于一般损失函数则是残差的近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法实现\n",
    "\n",
    "**导入相关库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**硬件与版本信息**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "ipywidgets 7.5.0\n",
      "matplotlib 3.1.0\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.2\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p ipywidgets,matplotlib,numpy,pandas,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**导入回归树**\n",
    "\n",
    "GBDT 的实现依赖于回归树，这里使用[决策树](https://libertydream.github.io/statistical_learning_method/notebook/5.decision_tree.html)中实现过的回归树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNode:\n",
    "    \"\"\"叶结点或内部结点\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_i=None, threshold=None, value=None,\n",
    "                 yes_subtree=None, no_subtree=None):\n",
    "        self.feature_i = feature_i\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.yes_subtree = yes_subtree\n",
    "        self.no_subtree = no_subtree   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    '''对选定特征进一步分类，获取独热数据集'''\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample:sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample:sample[feature_i] == threshold\n",
    "    \n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "    \n",
    "    return np.array([X_1, X_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"回归树和分类树的父类\"\"\"\n",
    "    \n",
    "    def __init__(self, min_split_num=2, min_impurity=1e-7):\n",
    "        \n",
    "        # 决策树根节点\n",
    "        self.root = None\n",
    "        \n",
    "        # 最小切分单位大小，样本数少于该值不进行进一步分割\n",
    "        self.min_split_num = min_split_num\n",
    "        \n",
    "        # 最小增益值，当划分带来的增益小于该值时停止生成\n",
    "        self.min_impurity = min_impurity\n",
    "        \n",
    "        # 计算增益，分类树下计算信息增益，回归树下计算方差缩减程度\n",
    "        self._cal_impurity = None\n",
    "        \n",
    "        # 计算叶子结点给出的预测 y\n",
    "        self._cal_leaf_val = None\n",
    "        \n",
    "        # y 是否经过 one-hot 编码，默认没有(one-dim)\n",
    "        self.one_dim = None\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.one_dim = len(np.shape(y_train)) == 1\n",
    "        self.root = self.__build_tree(X_train, y_train)\n",
    "    \n",
    "    def __build_tree(self, X_train, y_train):\n",
    "        \"\"\"递归创建一棵决策树\n",
    "        \n",
    "        对 X 按特征进行划分，比对不同划分选择下的误差以进行最优分类\n",
    "        \"\"\"\n",
    "        max_impurity = 0\n",
    "        best_criteria = None  # 最优划分特征\n",
    "        best_sets = None  # 最优划分形成的子集集合\n",
    "        \n",
    "        # 拼接成习惯的训练集形式\n",
    "        if len(np.shape(y_train)) == 1:\n",
    "            y_train = np.expand_dims(y_train, axis=1)\n",
    "        train_set = np.concatenate((X_train, y_train),axis=1)\n",
    "        \n",
    "        n_samples, n_features = np.shape(X_train)\n",
    "        \n",
    "        if n_samples > self.min_split_num: # 停止条件之一\n",
    "            \n",
    "            for feature_i in range(n_features):\n",
    "                \n",
    "                # 当前特征可取哪些值\n",
    "                feature_values = np.expand_dims(X_train[:,feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                \n",
    "                # 计算以当前特征为划分标准时对应的误差\n",
    "                for threshold in unique_values:\n",
    "                    \n",
    "                    X_y1, X_y2 = divide_on_feature(train_set, feature_i, threshold)\n",
    "                    \n",
    "                    if len(X_y1) > 0 and len(X_y2) > 0:  # 还有划分的必要\n",
    "                        y1 = X_y1[:, n_features:]\n",
    "                        y2 = X_y2[:, n_features:]\n",
    "                        \n",
    "                        impurity = self._cal_impurity(y_train, y1, y2)\n",
    "                        \n",
    "                        if impurity > max_impurity:\n",
    "                            max_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\":feature_i, \"threshold\":threshold}\n",
    "                            best_sets = {\n",
    "                                \"yes_X\": X_y1[:, : n_features],\n",
    "                                \"yes_y\": X_y1[:, n_features : ],\n",
    "                                \"no_X\": X_y2[:, : n_features],\n",
    "                                \"no_y\": X_y2[:, n_features : ]\n",
    "                            }\n",
    "                            \n",
    "                # 还值得继续生成树\n",
    "                if max_impurity > self.min_impurity:\n",
    "                \n",
    "                    yes_subtree = self.__build_tree(best_sets[\"yes_X\"],best_sets[\"yes_y\"])\n",
    "                    no_subtree = self.__build_tree(best_sets[\"no_X\"],best_sets[\"no_y\"])\n",
    "                    return DNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\"threshold\"],\n",
    "                            yes_subtree=yes_subtree, no_subtree=no_subtree)\n",
    "        \n",
    "        # 停止划分，已经成为叶子结点了，计算此时的预测输出\n",
    "        leaf_value = self._cal_leaf_val(y_train)\n",
    "            \n",
    "        return DNode(value=leaf_value)\n",
    "    \n",
    "    def predict_value(self, x, tree=None):\n",
    "        '''树tree对样本x的预测,递归实现'''\n",
    "        \n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        \n",
    "        # 抵达叶子结点，给出预测\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        # 还在内部结点，选择前进方向\n",
    "        feature_value = x[tree.feature_i]\n",
    "        goto = tree.yes_subtree\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value < tree.threshold:\n",
    "                goto = tree.no_subtree\n",
    "        elif feature_value != tree.threshold:\n",
    "            goto = tree.no_subtree\n",
    "        \n",
    "        return self.predict_value(x, goto)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = [self.predict_value(sample) for sample in X_test]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_var(X):\n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "    n_samples = np.shape(X)[0]\n",
    "    var = (1 / n_samples) * np.diag((X-mean)).T.dot(X - mean)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressDT(DecisionTree):\n",
    "    \n",
    "    def __cal_var_reduct(self, y_train, y1, y2):\n",
    "        \n",
    "        var_tot = cal_var(y_train)\n",
    "        var_1 = cal_var(y1)\n",
    "        var_2 = cal_var(y2)\n",
    "        \n",
    "        frac_1 = len(y1) / len(y_train)\n",
    "        frac_2 = len(y2) / len(y_train)\n",
    "        \n",
    "        var_reduct = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "        \n",
    "        return sum(var_reduct)\n",
    "    \n",
    "    def __mean(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self._cal_impurity = self.__cal_var_reduct\n",
    "        self._cal_leaf_val = self.__mean\n",
    "        super(RegressDT, self).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**辅助类设计**\n",
    "\n",
    "GBDT 本身是回归树，但是输出的连续值既可以用于连续值预测，也可以用于分类。两类任务对应于平方损失和对数损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareLoss(object):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def loss(self, y_true, y_pred):\n",
    "        return 0.5 * np.power((y_true - y_pred),2)\n",
    "    \n",
    "    def gradient(self, y_true, y_pred):\n",
    "        return -(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def loss(self, y_true, p_pred):\n",
    "        \n",
    "        # 防止除 0\n",
    "        p_pred = np.clip(p_pred, 1e-10, 1 - 1e-10)\n",
    "        \n",
    "        return -y_true * np.log(p_pred) - (1-y_true)*np.log(1- p_pred)\n",
    "    \n",
    "    def gradient(self, y_true, p_pred):\n",
    "        \n",
    "        p_pred = np.clip(p_pred, 1e-10, 1 - 1e-10)\n",
    "        \n",
    "        return -(y_true / p_pred) + (1 - y_true) / (1 - p_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBDT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDT(object):\n",
    "    '''梯度提升分类树和梯度提升回归树的父类\n",
    "    \n",
    "    n_estimators:\n",
    "        学习器数量\n",
    "    learning_rate:\n",
    "        每一轮迭代步长\n",
    "    min_impurity:\n",
    "        收益阈值，低于该值时停止学习\n",
    "    min_split_num:\n",
    "        当叶子结点内样本数低于该值，停止学习\n",
    "    regression:\n",
    "        判断当前是回归还是分类问题    \n",
    "    '''\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.3, \n",
    "                 min_split_num=2, min_impurity=1e-5, regression=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_split_num = min_split_num\n",
    "        self.min_impurity = min_impurity\n",
    "        self.regression = regression\n",
    "        \n",
    "        # 选择损失函数\n",
    "        self.loss = SquareLoss()\n",
    "        if not regression:\n",
    "            self.loss = CrossEntropy()\n",
    "        \n",
    "        # 初始化学习器\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = RegressDT(min_split_num = self.min_split_num, \n",
    "                             min_impurity = self.min_impurity)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        # 初始化预测值为一常数\n",
    "        y_pred = np.full(y_train.shape, np.mean(y, axis=0))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            gradient = self.loss.gradient(y_train, y_pred)\n",
    "            self.trees[i].fit(X_train, gradient)\n",
    "            gain = self.trees[i].predict(X_train)\n",
    "            \n",
    "            # 负梯度更新\n",
    "            y_pred -= np.multiply(self.learning_rate, gain)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        y_pred = np.array([])\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            \n",
    "            gain = tree.predict(X_test)\n",
    "            gian = np.multiply(self.learning_rate, gain)\n",
    "            y_pred -= -gain if not y_pred.any() else y_pred - gain\n",
    "    \n",
    "        if not self.regression:\n",
    "            \n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            \n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTRegressor(GBDT):\n",
    "    def __init__(self, n_estimators = 25, learning_rate = 0.5, min_split_num = 2,\n",
    "                min_var_red = 1e-7):\n",
    "        super(GBDTRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                           min_impurit = min_var_red,\n",
    "                                            min_split_num = min_split_num,\n",
    "                                            regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(x, n_col = None):\n",
    "    '''独热编码'''\n",
    "    if not n_col:\n",
    "        n_col = np.amax(x) + 1\n",
    "    one_hot = np.zeros((x.shape[0], n_col))\n",
    "    one_hot[np.arange(x.shape[0]), x] = 1\n",
    "    return one_hot\n",
    "\n",
    "class GBDTClassifier(GBDT):\n",
    "    def __init__(self, n_estimators = 25, learning_rate = 0.5, min_split_num = 2,\n",
    "                min_info_gain = 1e-7):\n",
    "        super(GBDTClassifier, self).__init__(n_estimators = n_estimators,\n",
    "                                            learning_rate = learning_rate,\n",
    "                                            min_split_num = min_split_num,\n",
    "                                            min_impurity = min_info_gain,\n",
    "                                            regression = False)\n",
    "    def fit(self, X_train, y_train):\n",
    "        y_train = to_categorical(y_train)\n",
    "        super(GBDTClassifier, self).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "作者：Daniel Meng\n",
    "\n",
    "GitHub: [LibertyDream](https://github.com/LibertyDream)\n",
    "\n",
    "博客：[明月轩](https://libertydream.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
