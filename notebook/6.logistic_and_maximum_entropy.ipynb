{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归是必知必会的经典分类方法，最大熵则是概率模型学习的准则之一，推广到分类问题就是最大熵模型。两个模型都是对数线性模型。\n",
    "\n",
    "* [6.1 逻辑斯蒂回归模型](#6.1-逻辑斯蒂回归模型)\n",
    "  * [6.1.1 逻辑斯蒂分布](#6.1.1-逻辑斯蒂分布)\n",
    "  * [6.1.2 二项逻辑斯蒂回归模型](#6.1.2-二项逻辑斯蒂回归模型)\n",
    "  * [6.1.3 模型参数估计](#6.1.3-模型参数估计)\n",
    "  * [6.1.4 多项式逻辑斯蒂回归](#6.1.4-多项式逻辑斯蒂回归)\n",
    "* [6.2 最大熵模型](#6.2-最大熵模型)\n",
    "  * [6.2.1 最大熵原理](#6.2.1-最大熵原理)\n",
    "  * [6.2.2 最大熵模型的定义](#6.2.2-最大熵模型的定义)\n",
    "  * [6.2.3 最大熵模型的学习](#6.2.3-最大熵模型的学习)\n",
    "  * [6.2.4 极大似然估计](#6.2.4-极大似然估计)\n",
    "* [6.3 模型学习的最优化方法](#6.3-模型学习的最优化方法)\n",
    "  * [6.3.1 改进的迭代尺度法](#6.3.1-改进的迭代尺度法)\n",
    "  * [6.3.2 拟牛顿法](#6.3.2-拟牛顿法)\n",
    "* [算法实现](#算法实现)\n",
    "* [习题](#习题)\n",
    "\n",
    "\n",
    "\n",
    "# 6.1 逻辑斯蒂回归模型\n",
    "\n",
    "## 6.1.1 逻辑斯蒂分布\n",
    "\n",
    "逻辑斯蒂分布（logistic distribution）有如下分布函数和密度函数：\n",
    "$$\n",
    "\\begin{array}{l}{F(x)=P(X \\leqslant x)=\\frac{1}{1+\\mathrm{e}^{-(x-\\mu) / \\gamma}}} \\\\ {f(x)=F^{\\prime}(x)=\\frac{\\mathrm{e}^{-(x-\\mu) / \\gamma}}{\\gamma\\left(1+\\mathrm{e}^{-(x-\\mu) / \\gamma}\\right)^{2}}}\\end{array}\n",
    "$$\n",
    "$\\mu$ 是位置参数，$\\gamma$ 是形状参数\n",
    "\n",
    "分布函数 $F(x)$ 是 S 形曲线（sigmoid curve），并以点 $(\\mu, \\frac{1}{2})$ 为中心对称，即\n",
    "$$\n",
    "F(x+\\mu)-\\frac{1}{2}= -F(-x+\\mu)+\\frac{1}{2}\n",
    "$$\n",
    "形状参数 $\\gamma$ 越小，中心附近增长越快\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-09-22_logistics_distribution.png)\n",
    "\n",
    "## 6.1.2 二项逻辑斯蒂回归模型\n",
    "\n",
    "二项逻辑斯蒂回归模型（binomial logistic regression model）可以写成如下条件概率分布：\n",
    "$$\n",
    "\\begin{array}{l}{P(Y=1 | x)=\\frac{\\exp (w \\cdot x)}{1+\\exp (w \\cdot x)}} \\\\ {P(Y=0 | x)=\\frac{1}{1+\\exp (w \\cdot x)}}\\end{array}\n",
    "$$\n",
    "$Y$ 是输出，$w = (w^{(1)},w^{(2)},\\ldots,w^{(n)},b)^{T}$，$x = (x^{(1)},x^{(2)},\\ldots,x^{(n)},1)^{T}$，$b$ 是偏置\n",
    "\n",
    "一个事件的几率（odds）是指事件发生概率与不发生概率的比值 $\\frac{p}{1-p}$，事件对数几率（log odds）或 logit 函数为\n",
    "$$\n",
    "\\operatorname{logit}(p)=\\log \\frac{p}{1-p}\n",
    "$$\n",
    "带入逻辑斯蒂概率公式可得\n",
    "$$\n",
    "\\log \\frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \\cdot x\n",
    "$$\n",
    "可见逻辑斯蒂回归模型输出 $Y=1$ 的对数几率是输入 $x$ 的线性函数，这也是其称为对数线性模型的原因\n",
    "\n",
    "另一方面，逻辑斯蒂回归模型可以理解为将 $x$ 的线性函数转换成了概率，函数值越大，概率越接近1，反之，函数值越小，概率越接近 0。\n",
    "\n",
    "## 6.1.3 模型参数估计\n",
    "\n",
    "逻辑斯蒂回归模型学习时使用极大似然法进行参数估计。令\n",
    "$$\n",
    "P(Y=1|x) = \\pi(x),\\quad P(Y=0|x)=1 - \\pi(x)\n",
    "$$\n",
    "可得似然函数\n",
    "$$\n",
    "\\prod_{i=1}^{N}\\left[\\pi\\left(x_{i}\\right)\\right]^{y_{i}}\\left[1-\\pi\\left(x_{i}\\right)\\right]^{1-y_{i}}\n",
    "$$\n",
    "进而可得对数似然函数\n",
    "$$\n",
    "\\begin{aligned} L(w) &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\pi\\left(x_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\frac{\\pi\\left(x_{i}\\right)}{1-\\pi\\left(x_{i}\\right)}+\\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \\\\ &=\\sum_{i=1}^{N}\\left[y_{i}\\left(w \\cdot x_{i}\\right)-\\log \\left(1+\\exp \\left(w \\cdot x_{i}\\right)\\right]\\right.\\end{aligned}\n",
    "$$\n",
    "转换成为关于求解 $L(w)$ 最大值的最优化问题，通过梯度下降或者牛顿法得到参数估计值 $\\hat{w}$\n",
    "\n",
    "### 梯度求解\n",
    "\n",
    "$$\n",
    "\\begin{aligned} p^{\\prime}=f^{\\prime}(\\boldsymbol{w}) &=\\left(\\frac{1}{1+e^{-w^{T} \\boldsymbol{x}}}\\right)^{\\prime} \\\\ &=-\\frac{1}{\\left(1+e^{-w^{T} \\boldsymbol{x}}\\right)^{2}} \\cdot\\left(1+e^{-\\boldsymbol{w}^{T} \\boldsymbol{x}}\\right)^{\\prime} \\\\ &=-\\frac{1}{\\left(1+e^{-\\boldsymbol{w}^{T} \\boldsymbol{x}}\\right)^{2}} \\cdot e^{-\\boldsymbol{w}^{T} \\boldsymbol{x}} \\cdot\\left(-\\boldsymbol{w}^{T} \\boldsymbol{x}\\right)^{\\prime} \\\\ &=-\\frac{1}{\\left(1+e^{-w^{T} \\boldsymbol{x}}\\right)^{2}} \\cdot e^{-\\boldsymbol{w}^{T} \\boldsymbol{x}} \\cdot(-\\boldsymbol{x}) \\\\ \n",
    "&=\\frac{1}{1+e^{-w^{T} \\boldsymbol{x}}} \\cdot \\frac{e^{-\\boldsymbol{w}^{T} \\boldsymbol{x}}}{1+e^{-w^{T} \\boldsymbol{x}}} \\cdot \\boldsymbol{x} \n",
    "\\\\&=\\frac{1}{1+e^{-w^{T} \\boldsymbol{x}}} \\cdot\\left(1- \\frac{1}{1+e^{-w^{T} \\boldsymbol{x}}}\\right) \\cdot \\boldsymbol{x} \n",
    "\\\\ &=p(1-p) \\boldsymbol{x} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由此可知，$(1-p)^{\\prime} = -p(1-p)\\boldsymbol{x} $\n",
    "$$\n",
    "\\begin{aligned} \\nabla F(\\boldsymbol{w}) &=\\nabla\\left(\\sum_{n=1}^{N}\\left(y_{n} l n(p)+\\left(1-y_{n}\\right) \\ln (1-p)\\right)\\right) \\\\ &=\\sum_{n=1}^{N}\\left(y_{n} \\ln ^{\\prime}(p)+\\left(1-y_{n}\\right) \\ln ^{\\prime}(1-p)\\right) \\\\ &=\\sum_{n=1}^{N}\\left(\\left(y_{n} \\frac{1}{p} p^{\\prime}\\right)+\\left(1-y_{n}\\right) \\frac{1}{1-p}(1-p)^{\\prime}\\right) \\\\ &=\\sum_{n=1}^{N}\\left(y_{n}(1-p) \\boldsymbol{x}_{n}-\\left(1-y_{n}\\right) p \\boldsymbol{x}_{n}\\right) \\\\ &=\\sum_{n=1}^{N}\\left(y_{n}-p\\right) \\boldsymbol{x}_{n} \\end{aligned}\n",
    "$$\n",
    "\n",
    "## 6.1.4 多项式逻辑斯蒂回归\n",
    "\n",
    "多项式逻辑斯蒂回归（multi-nominal logistic regression method）用于多分类任务。参数估计方法和二项逻辑斯蒂回归相同。\n",
    "$$\n",
    "P(Y=k | x)=\\frac{\\exp \\left(w_{k} \\cdot x\\right)}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}\\\\\n",
    "P(Y=K | x)=\\frac{1}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}\n",
    "$$\n",
    "\n",
    "# 6.2 最大熵模型\n",
    "\n",
    "## 6.2.1 最大熵原理\n",
    "\n",
    "最大熵模型（maximum entropy model）是由最大熵原理推导得到的。最大熵原理是概率模型的学习准则之一，认为在给定约束条件下的所有可能模型中，熵最大的模型是最好的模型。\n",
    "\n",
    "随机变量 $X$ 的熵表示为 $H(P)=-\\sum_{x} P(x) \\log P(x)$。熵都满足不等式：\n",
    "$$\n",
    "0 \\leqslant H(P) \\leqslant \\log |X|\n",
    "$$\n",
    "$|X|$ 表示 $X$ 的取值个数，当且仅当 $X$ 服从均匀分布，各个取值机会均等的时候，右边等号成立，熵最大。\n",
    "\n",
    "另一个角度看，概率模型都要满足既有事实，也就是约束条件。在没有更多信息补充时，默认那些不确定的部分是等可能的是比较稳妥的选择。最大熵原理通过熵的最大化表示等可能性。\n",
    "\n",
    "## 6.2.2 最大熵模型的定义\n",
    "\n",
    "最大熵原理是统计学习的一般原理，应用到分类任务上就得到了最大熵模型。\n",
    "\n",
    "首先思考一下模型的约束条件。给定数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$，我们可以得到经验分布 \n",
    "$$\n",
    "\\begin{array}{l}{\\tilde{P}(X=x, Y=y)=\\frac{\\nu(X=x, Y=y)}{N}} \\\\ {\\tilde{P}(X=x)=\\frac{\\nu(X=x)}{N}}\\end{array}\n",
    "$$\n",
    "$v(x)$ 表示样本中 $x$ 出现的频次。我们要的是模型预测的结果和事实相符，也就是对于样本数据模型预测的期望值 $E_{\\tilde{P}}(f)$ 和我们直观观测到的期望值 $E_{P}(f)$ 应该相同。\n",
    "$$\n",
    "E_{\\tilde{P}}(f)=\\sum_{x, y} \\tilde{P}(x, y) f(x, y) \\\\\n",
    "E_{P}(f)=\\sum_{x, y} \\tilde{P}(x) P(y | x) f(x, y)\n",
    "$$\n",
    "其中 $f(x,y)$ 是特征函数（feature function）\n",
    "$$\n",
    "f(x, y)=\\left\\{\\begin{array}{ll}{1,} & x 与 y满足某一事实 \\\\ {0,} & 否则\\end{array}\\right.\n",
    "$$\n",
    "所以两个期望相同就是约束条件。最大熵模型就可以表示成，在满足所有约束条件的模型集合 $\\mathcal{C}$ 里\n",
    "$$\n",
    "\\mathcal{C} \\equiv\\left\\{P \\in \\mathcal{P} | E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\\right\\}\n",
    "$$\n",
    "选择条件熵 $H(P)$ 最大的模型\n",
    "$$\n",
    "H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y | x) \\log P(y | x)\n",
    "$$\n",
    "其中对数为自然对数\n",
    "\n",
    "## 6.2.3 最大熵模型的学习\n",
    "\n",
    "最大熵模型的学习等价于约束最优化问题：\n",
    "$$\n",
    "\\begin{array}{ll}{\\max _{P \\in \\mathbb{C}}} & {H(P)=-\\sum_{x, y} \\tilde{P}(x) P(y | x) \\log P(y | x)} \\\\ {\\text { s.t. }} & {E_{P}\\left(f_{i}\\right)=E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n} \\\\ {} & {\\sum_{y} P(y | x)=1}\\end{array}\n",
    "$$\n",
    "改成就最小值的形式：\n",
    "$$\n",
    "\\begin{array}{ll}{\\min _{P \\in \\mathbb{C}}} & {-H(P)=\\sum_{x, y} \\tilde{P}(x) P(y | x) \\log P(y | x)} \\\\ {\\text { s.t. }} & {E_{P}\\left(f_{i}\\right)-E_{\\tilde{P}}\\left(f_{i}\\right)=0, \\quad i=1,2, \\cdots, n} \\\\ {} & {\\sum_{y} P(y | x)=1}\\end{array}\n",
    "$$\n",
    "等式约束下的最优化问题使用拉格朗日乘数法，引入拉格朗日乘子 $w_{0},w_{1},\\ldots,w_{n}$，得到拉格朗日函数 $L(P,w)$\n",
    "$$\n",
    "\\begin{aligned} L(P, w) \\equiv &-H(P)+w_{0}\\left(1-\\sum_{y} P(y | x)\\right)+\\sum_{i=1}^{n} w_{i}\\left(E_{\\tilde{P}}\\left(f_{i}\\right)-E_{P}\\left(f_{i}\\right)\\right) \\\\=& \\sum_{x, y} \\tilde{P}(x) P(y | x) \\log P(y | x)+w_{0}\\left(1-\\sum_{y} P(y | x)\\right)+\\\\ & \\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P(y | x) f_{i}(x, y)\\right) \\end{aligned}\n",
    "$$\n",
    "原始最优化问题 $\\min _{P \\in \\mathbf{C}} \\max _{w} L(P, w)$ 的对偶问题是 $\\max _{\\boldsymbol{w}} \\min _{P \\in \\mathbf{C}} L(P, w)$。因为函数 $L(P,w)$ 是 $P(y|x)$ 的凸函数，所以原始问题和对偶问题的解是等价的。\n",
    "\n",
    "先求内部最小化问题，令 $\\Psi(w)=\\min _{P \\in \\mathbf{C}} L(P, w)=L\\left(P_{w}, w\\right)$ ，$\\Psi(w)$ 称为对偶函数。 $L(P,w)$ 对 $P(y|x)$ 求偏导得\n",
    "$$\n",
    "\\frac{\\partial L(P, w)}{\\partial P(y | x)} =\\sum_{x, y} \\tilde{P}(x)(\\log P(y | x)+1)-\\sum_{y} w_{0}-\\sum_{x, y}\\left(\\tilde{P}(x) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n",
    "$$\n",
    "因为$\\sum_{x} \\tilde{P}(x) = 1$ ，所以\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial L(P, w)}{\\partial P(y | x)} &=\\sum_{x, y} \\tilde{P}(x)(\\log P(y | x)+1)-\\sum_{x} \\tilde{P}(x) \\sum_{y} w_{0}-\\sum_{x, y}\\left(\\tilde{P}(x) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\\\\\n",
    "&=\\sum_{x, y} \\tilde{P}(x)\\left(\\log P(y | x)+1-w_{0}-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \n",
    "\\end{aligned}\n",
    "$$\n",
    "令偏导数等于 0，当 $\\tilde{P}(x)>0$，解得\n",
    "$$\n",
    "P(y | x)=\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)+w_{0}-1\\right)=\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\exp \\left(1-w_{0}\\right)}\n",
    "$$\n",
    "又因为 $\\sum_{y} P(y | x)=1$，所以\n",
    "$$\n",
    "\\sum_{y} P(y | x)=\\sum_{y}\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\exp \\left(1-w_{0}\\right)} = 1 \\\\\n",
    "\\frac{\\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\exp \\left(1-w_{0}\\right)} = 1\n",
    "$$\n",
    "得到 $\\exp(1-w_{0}) =\\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)$ 带入 $P(y|x)$ 的解中，得到最优解\n",
    "$$\n",
    "P_{w}=\\arg \\min _{P \\in \\mathbf{C}} L(P, w)=P_{w}(y | x)= \\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}\n",
    "$$\n",
    "其中 $w_{i}$ 是权重，$f_{i}(x,y)$ 是特征函数，$ Z_{w}(x) = \\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)$ 叫做规范化因子。\n",
    "\n",
    "随后求解外部的最大化问题 $\\max _{w} \\Psi(w)$，$w$ 是参数向量。解得  $w^{*}=\\arg \\max _{w} \\Psi(w)$。所以此时，最大熵模型的学习变成了对偶函数 $\\Psi(w)$ 的极大化问题。\n",
    "\n",
    "## 6.2.4 极大似然估计\n",
    "\n",
    "证明：对偶函数的极大化等价于最大熵模型的极大似然估计\n",
    "\n",
    "在经验概率分布$\\tilde{P}(X,Y)$ 上，条件概率分布 $P(Y|X)$ 的极大似然函数 $L_{1}$ 为\n",
    "$$\n",
    "L_{1}=\\prod_{i=1}^{n} P\\left(y_{i} | x_{i}\\right)=P\\left(y_{1} | x_{1}\\right) \\cdots P\\left(y_{n} | x_{n}\\right)\n",
    "$$\n",
    "$n$ 个样例里难免会有相同样例$(x^{i},y^{j})$，于是 $L_{1}$ 可以写为\n",
    "$$\n",
    "{L_{1}=P\\left(y^{1} | x^{1}\\right)^{v\\left(x^{1}, y^{1}\\right)} \\cdots P\\left(y^{r} | x^{m}\\right)^{v\\left(x^{m}, y^{r}\\right)}}=\\prod_{y, x} P(y | x)^{v(x, y)}\n",
    "$$\n",
    "$v$ 表示共现数量，而此时 $L_1$ 的极大化等价于下式极大化\n",
    "$$\n",
    "L_{2}=\\prod_{y, x} P(y | x)^{\\frac{v(x, y)}{N}}=\\prod_{y, x} P(y | x)^{\\tilde{P}(x, y)}\n",
    "$$\n",
    "当条件概率分布为上面所求的最大熵模型 $P_{w}$ 时，带入 $L_2$ 并求对数，得到对数似然函数 $L_{\\tilde{P}}(P_{w})$\n",
    "$$\n",
    "\\begin{aligned} L_{\\tilde{P}}\\left(P_{w}\\right)&=\\log \\prod_{x, y} P(y | x)^{\\tilde{P}(x, y)}=\\sum_{x, y} \\tilde{P}(x, y) \\log P(y | x)\\\\ &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x, y) \\log Z_{w}(x) \\\\ \n",
    "&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\log Z_{w}(x)\\sum_{y}\\tilde{P}(x, y) \\\\\n",
    "&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x) \\end{aligned}\n",
    "$$\n",
    "再看对偶函数 $\\Psi(w)=L\\left(P_{w}, w\\right)$\n",
    "$$\n",
    "\\begin{aligned}\\Psi(w)&=\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) \\log P_{w}(y | x)+\\sum_{i=1}^{n} w_{i}\\left(\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) f_{i}(x, y)\\right) \\\\ &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)+\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x)\\left(\\log P_{w}(y | x)-\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\ &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) \\log Z_{w}(x)\\\\\n",
    "&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\\sum_{y}P_{w}(y | x) \\\\\n",
    "&=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "可见 $\\Psi(w)=L_{\\tilde{P}}\\left(P_{w}\\right)$，于是最大熵模型学习中对偶函数的极大化等价于最大熵模型的极大似然估计\n",
    "\n",
    "### 最大熵模型与逻辑斯蒂模型的关系\n",
    "\n",
    "当分类任务为二分类 $Y=\\{y_1,y_2\\}$，且特征函数为\n",
    "$$\n",
    "f_{i}(\\mathbf{x}, y)=\\left\\{\\begin{array}{ll}{x_{i},} & {\\text { if } y=y_{1}} \\\\ {0,} & 其他\\end{array}\\right.\n",
    "$$\n",
    "有\n",
    "$$\n",
    "P\\left(y_{1} | \\mathbf{x}\\right)=\\frac{e^{\\sum_{i=1}^{n} w_{i} x_{i}}}{1+e^{\\sum_{i=1}^{n} w_{i} x_{i}}}=\\frac{e^{\\mathbf{w}^{T} \\mathbf{x}}}{1+e^{\\mathbf{w}^{T} \\mathbf{x}}} \\\\\n",
    "P\\left(y_{0} | \\mathbf{x}\\right)=\\frac{1}{1+e^{\\mathbf{w}^{T} \\mathbf{x}}}\n",
    "$$\n",
    "此时最大熵模型退化成为逻辑斯蒂回归模型\n",
    "\n",
    "# 6.3 模型学习的最优化方法\n",
    "\n",
    "逻辑斯蒂回归和最大熵模型都可以归结为似然函数为目标函数的最优化问题，且目标函数还是凸函数。所以优化方法有很多，常用的有改良迭代尺度法，梯度下降法，牛顿法或拟牛顿法。后两者收敛更快些（二阶导）\n",
    "\n",
    "## 6.3.1 改进的迭代尺度法\n",
    "\n",
    "改进的迭代尺度法（improved iterative scaling, IIS）是一种求解最大熵模型的最优化算法。因为要极大化似然函数，该算法思想是，如果能找到一种更新参数向量 $w$ 的方法 $\\tau : w \\rightarrow w+\\delta$，因为加了 $\\delta$ 使得似然函数变大，那么不断重复这个过程就能找到似然函数的最大值。\n",
    "\n",
    "对数似然函数变大可以表示为增量 $L(w+\\delta)-L(w) > 0$，具体的\n",
    "$$\n",
    "\\begin{aligned} L(w+\\delta)-L(w) &=\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w+\\delta}(y | x)-\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w}(y | x) \\\\ &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log \\frac{Z_{w+\\delta}(x)}{Z_{w}(x)} \\end{aligned}\n",
    "$$\n",
    "其中$P_w(y|x)$是最大熵模型，$Z_{w}(x)$ 是规范化因子 \n",
    "$$\n",
    "P_{w}(y | x)=\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\sum_{y}\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}=\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{Z_{w}(x)}\n",
    "$$\n",
    "利用不等式 $-\\log \\alpha \\geqslant 1- \\alpha,\\alpha > 0$，可得\n",
    "$$\n",
    "\\begin{aligned} L(w+\\delta)-L(w) & \\geqslant \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\frac{Z_{w+\\delta}(x)}{Z_{w}(x)} \\\\ &=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y | x) \\exp \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y) \\end{aligned}\n",
    "$$\n",
    "这样就得到了增量下界 $A(\\delta|w)$，如果下界越大，似然函数也就越大，但不等式右边的 $\\delta$ 是向量，变量太多。所以 IIS 试图只优化一个变量 $\\delta_i$，其他变量不动，获取一个更低但好计算的下界\n",
    "\n",
    "引入 $f^\\#(x,y) = \\sum_i f_i(x,y)$，表示所有特征为$(x,y)$的样本数。这样\n",
    "$$\n",
    "\\begin{aligned} A(\\delta | w)=& \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y | x) \\exp \\left(f^{\\#}(x, y) \\sum_{i=1}^{n} \\frac{\\delta_{i} f_{i}(x, y)}{f^{\\#}(x, y)}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "### Jensen 不等式\n",
    "\n",
    "过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方，即\n",
    "$$\n",
    "t f\\left(x_{1}\\right)+(1-t) f\\left(x_{2}\\right) \\geqslant f\\left(t x_{1}+(1-t) x_{2}\\right), 0 \\leqslant t \\leqslant 1\n",
    "$$\n",
    "泛化形式为，对点集 $\\{x_i\\}$，如果 $\\lambda_i \\geqslant 0,\\sum_i \\lambda_i = 1$，则有 $f\\left(\\sum_{i=1}^{M} \\lambda_{i} x_{i}\\right) \\leqslant \\sum_{i=1}^{M} \\lambda_{i} f\\left(x_{i}\\right)$\n",
    "\n",
    "将 $\\lambda$ 视作概率分布的话，那么就可以写做 $f(E[x]) \\leqslant E[f(x)]$\n",
    "\n",
    "而指数函数是凸函数，同时 $\\frac{f_{i}(x, y)}{f^{\\#}(x, y)} \\geqslant 0$， $\\sum_{i=1}^{n} \\frac{f_{i}(x, y)}{f^{\\#}(x, y)}=1$。所以\n",
    "$$\n",
    "A(\\delta | w) \\geqslant \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y | x) \\sum_{i=1}^{n}\\left(\\frac{f_{i}(x, y)}{f^{\\#}(x, y)}\\right) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right) \n",
    "$$\n",
    "这样就得到了新的下界，记不等式右端为 $B(\\delta|w)$，对 $\\delta_i$ 求偏导得\n",
    "$$\n",
    "\\frac{\\partial B(\\delta | w)}{\\partial \\delta_{i}}=\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y | x) f_{i}(x, y) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)\n",
    "$$\n",
    "令导数为 0 ，得到只关于 $\\delta_i$ 的方程\n",
    "$$\n",
    "\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) f_{i}(x, y) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)=E_{\\tilde{P}}\\left(f_{i}\\right)\\tag{1}\n",
    "$$\n",
    "这样我们就可以计算 $\\delta$ ，更新 $w$ 了。到此，改进的迭代尺度算法（IIS）可以描述如下：\n",
    "\n",
    "1. 初始化 $w_i = 0$，$i \\in \\{1,2,\\ldots,n\\}$\n",
    "2. 通过（1）式计算 $\\delta_i$，更新 $w_{i} \\leftarrow w_{i}+\\delta_{i}$\n",
    "3. 如果不是所有的 $w_i$ 都收敛，重复第二步\n",
    "\n",
    "当 $f^{\\#}(x, y)=M$，$\\delta_i$ 可表示为 $\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{P}}\\left(f_{i}\\right)}{E_{P}\\left(f_{i}\\right)}$\n",
    "\n",
    "当 $f^{\\#}(x, y)$ 不是常数，用$g(\\delta_i)=0$ 表示公式（1），使用牛顿法迭代求解得到 $g(\\delta^*)=0$，迭代公式为\n",
    "$$\n",
    "\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)}\n",
    "$$\n",
    "\n",
    "## 6.3.2 拟牛顿法\n",
    "\n",
    "牛顿法和拟牛顿法都是求解无约束最优化问题的方法，有收敛速度快的特点。牛顿法是迭代算法，每一步要求解目标函数的海森矩阵的逆矩阵，计算较为复杂，拟牛顿法通过正定矩阵近似海森矩阵（BFGS）或其逆矩阵（DFP)，简化了这一过程\n",
    "\n",
    "具体的对于二阶连续可导的$f(x)$，迭代第 $k$ 次的值记为 $x^{(k)}$，在该点附近二阶泰勒展开\n",
    "$$\n",
    "f(x)=f\\left(x^{(k)}\\right)+g_{k}^{\\mathrm{T}}\\left(x-x^{(k)}\\right)+\\frac{1}{2}\\left(x-x^{(k)}\\right)^{\\mathrm{T}} H\\left(x^{(k)}\\right)\\left(x-x^{(k)}\\right)\n",
    "$$\n",
    "这里 $g_{k}=g\\left(x^{(k)}\\right)=\\nabla f\\left(x^{(k)}\\right)$ 是梯度向量，$H$为海森矩阵（Hessian matrix）\n",
    "$$\n",
    "H(x)=\\left[\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right]_{n \\times n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla f(x)=g_{k}+H_{k}\\left(x-x^{(k)}\\right)\\tag{1}\n",
    "$$\n",
    "\n",
    "其中 $H_k=H(x^{(k)})$。极值存在的必要条件是极值点梯度为0，即 $\\nabla f(x)=0$，假设 $x^{(k+1)}$ 满足  $\\nabla f(x^{(k+1)})=0$,即\n",
    "$$\n",
    "g_{k}+H_{k}\\left(x^{(k+1)}-x^{(k)}\\right)=0\n",
    "$$\n",
    "可得\n",
    "$$\n",
    "x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}\n",
    "$$\n",
    "这就是牛顿法的更新公式。我们采用BFGS算法，尝试用正定矩阵 $B_k$ 逼近矩阵 $H$，我们在式(1)中取$x=x^{(k+1)}$，得\n",
    "$$\n",
    "g_{k+1}-g_{k}=H_{k}\\left(x^{(k+1)}-x^{(k)}\\right)\n",
    "$$\n",
    "记 $y_{k}=g_{k+1}-g_{k},\\delta_{k}=x^{(k+1)}-x^{(k)}$，得到 $y_k=H_k\\delta_k$，这称为拟牛顿条件，因为要用 $B_k$ 逼近 $H$，所以得到了BFGS算法下的拟牛顿条件\n",
    "$$\n",
    "B_{k+1} \\delta_{k}=y_{k}\n",
    "$$\n",
    "这样每次迭代中就是更新矩阵 $B_{k+1} = B_k + \\Delta B_{k}$，具体有多种实现，这里选用 Broyden 类拟牛顿法，该方法假设\n",
    "$$\n",
    "\\begin{array}{c}{B_{k+1}=B_{k}+P_{k}+Q_{k}} \\\\ {B_{k+1} \\delta_{k}=B_{k} \\delta_{k}+P_{k} \\delta_{k}+Q_{k} \\delta_{k}}\\end{array}\n",
    "$$\n",
    "$P_{k},Q_{k}$ 是待定矩阵。考虑要满足拟牛顿条件，可使\n",
    "$$\n",
    "\\begin{array}{c}{P_{k} \\delta_{k}=y_{k}} \\\\ {Q_{k} \\delta_{k}=-B_{k} \\delta_{k}}\\end{array}\n",
    "$$\n",
    "这样就得到了BFGS算法的迭代公式\n",
    "$$\n",
    "B_{k+1}=B_{k}+\\frac{y_{k} y_{k}^{\\mathrm{T}}}{y_{k}^{\\mathrm{T}} \\delta_{k}}-\\frac{B_{k} \\delta_{k} \\delta_{k}^{\\mathrm{T}} B_{k}}{\\delta_{k}^{\\mathrm{T}} B_{k} \\delta_{k}}\n",
    "$$\n",
    "其中为了确保右侧是方阵，先右乘 $y_k^T$，$(B_{k} \\delta_{k})^T$，$B_k$是对称正定矩阵，转置保持不变\n",
    "\n",
    "对于最大熵模型，目标函数为\n",
    "$$\n",
    "\\min _{w \\in \\mathbb{R}^{n}} f(w)=\\sum_{x} \\tilde{P}(x) \\log \\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)-\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\n",
    "$$\n",
    "梯度\n",
    "$$\n",
    "g(w)=\\left(\\frac{\\partial f(w)}{\\partial w_{1}}, \\frac{\\partial f(w)}{\\partial w_{2}}, \\cdots, \\frac{\\partial f(w)}{\\partial w_{n}}\\right)^{\\mathrm{T}}\\\\\n",
    "\\frac{\\partial f(w)}{\\partial w_{i}}=\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) f_{i}(x, y)-E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\n",
    "$$\n",
    "BFGS算法描述如下：\n",
    "\n",
    "1. 选定初始点 $w^{(0)}$，取正定对称矩阵 $B_0$，$k=0$\n",
    "2. 计算 $g_k = g(w^{(k)})$，如果$\\left\\|g_{k}\\right\\|<\\varepsilon$ ，停止计算，得到 $w^* = w^{(k)}$，否则转3\n",
    "3. 由$B_{k} p_{k}=-g_{k}$，求得$p_k$\n",
    "4. 一维搜索，找到 $\\lambda_k$ 使得 $f\\left(w^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(w^{(k)}+\\lambda p_{k}\\right)$\n",
    "5. 更新 $w^{(k+1)}=w^{(k)}+\\lambda_{k} p_{k}$\n",
    "6. 计算 $g_{k+1} = g(w^{(k+1)})$，如果 $\\left\\|g_{k+1}\\right\\|<\\varepsilon$，停止计算，得 $w^* = w^{(k+1)}$，否则求解 $B_{k+1}$\n",
    "7. 更新 $k = k + 1$，转3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法实现\n",
    "\n",
    "**导入相关库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**硬件与版本信息**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "ipywidgets 7.5.0\n",
      "matplotlib 3.1.0\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.2\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p ipywidgets,matplotlib,numpy,pandas,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**逻辑回归**\n",
    "\n",
    "使用梯度下降法进行似然估计，转换成标准最优化形式就是求 $\\min -L(\\boldsymbol{w})$，求导并取负梯度为更新方向，即 $(\\boldsymbol{y}-\\boldsymbol{p})\\cdot \\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_():\n",
    "        \n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _init_param(self, X):\n",
    "        num_features = np.shape(X)[1]\n",
    "        limit = 1 / math.sqrt(num_features)\n",
    "        self.param = np.random.uniform(-limit, limit, (num_features,))\n",
    "        \n",
    "    def fit(self, X_train, y_train, iterations = 200):\n",
    "        self._init_param(X_train)\n",
    "        for i in range(iterations):\n",
    "            y_pred = self.__sigmoid(X_train.dot(self.param))\n",
    "            self.param -= self.learning_rate * (y_train - y_pred).dot(X_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = np.round(self.__sigmoid(X_test.dot(self.param))).astype(int)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最大熵模型**\n",
    "\n",
    "这里采用通用迭代尺度法（Generalized Iterative Scaling，GIS）作为最优化方法，GIS 可以概括为这样几个步骤：\n",
    "\n",
    "1. 假定第零次迭代的初始模型为等概率均匀分布\n",
    "2. 用第N次迭代的模型估算每种特征在训练集中的分布。超过实际值调小模型参数，反之，调大\n",
    "3. 重复步骤2直到收敛\n",
    "\n",
    "具体的，更新公式为\n",
    "\n",
    "$$\n",
    "w_{i}^{(t+1)}=w_{i}^{(t)}+\\frac{1}{C} \\log \\frac{E_{\\hat{p}}\\left(f_{i}\\right)}{E_{p^{(n)}}\\left(f_{i}\\right)}, i \\in\\{1,2, \\ldots, n\\}\n",
    "$$\n",
    "\n",
    "其中$\\frac{1}{C}$类似于学习率，$C=\\max _{x, y} \\sum_{i=1}^{n} f_{i}(x, y)$，代表包含特征最多的样本所含有的特征数\n",
    "\n",
    "GIS 迭代时间长，且不稳定，64位计算机上都可能溢出，这里只用于理解最大熵模型的内在运行机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MaxEntropy(object):\n",
    "    '''\n",
    "        基于 GIS 的最大熵模型\n",
    "    \n",
    "    _samples:\n",
    "        样本集\n",
    "    _Y:\n",
    "        标签种类集合\n",
    "    _numXY:\n",
    "        记录(x,y)出现次数的字典\n",
    "    _N: \n",
    "        样本数\n",
    "    _Ep_:\n",
    "        经验估计的期望\n",
    "    _xyID:\n",
    "        记录(x,y)索引号的字典\n",
    "    _n:\n",
    "        不同(x,y)共现对的个数\n",
    "    _C:\n",
    "        样本中特征数最多的样本所含有的特征数\n",
    "    _IDxy:\n",
    "        与_xyID相反，通过索引找(x,y)\n",
    "    _w:\n",
    "        本轮训练的权重\n",
    "    _epsilon:\n",
    "        收敛条件\n",
    "    _lastw:\n",
    "        上一轮的权重\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, epsilon=0.01):\n",
    "        self._samples = []\n",
    "        self._Y = set()\n",
    "        self._numXY = {}\n",
    "        self._N = 0\n",
    "        self._Ep_ = []\n",
    "        self._xyID = {}\n",
    "        self._n = 0\n",
    "        self._C = 0\n",
    "        self._IDxy = {}\n",
    "        self._w = []\n",
    "        self._epsilon = epsilon\n",
    "        self._lastw = []\n",
    "        \n",
    "    def fit(self, dataset):\n",
    "        self._samples = deepcopy(dataset)\n",
    "        for items in self._samples:\n",
    "            y = items[0]\n",
    "            X = items[1:]\n",
    "            self._Y.add(y) # np.unique(y)\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    self._numXY[(x, y)] += 1\n",
    "                else:\n",
    "                    self._numXY[(x, y)] = 1\n",
    "\n",
    "        self._N = len(self._samples)\n",
    "        self._n = len(self._numXY)\n",
    "        self._C = max([len(sample) - 1 for sample in self._samples])\n",
    "        self._w = [0] * self._n\n",
    "        self._lastw = self._w[:]\n",
    "\n",
    "        self._Ep_ = [0] * self._n\n",
    "        for i, xy in enumerate(self._numXY):\n",
    "            self._Ep_[i] = self._numXY[xy] / self._N\n",
    "            self._xyID[xy] = i\n",
    "            self._IDxy[i] = xy\n",
    "    \n",
    "    def _Zx(self, X): \n",
    "        '''计算规范化因子 Z(x)'''\n",
    "        zx = 0\n",
    "        for y in self._Y:\n",
    "            ss = 0\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    ss += self._w[self._xyID[(x, y)]]\n",
    "            zx += math.exp(ss)\n",
    "        return zx\n",
    "    \n",
    "    def _model_pyx(self, y, X):\n",
    "        '''计算条件概率 p(y|x)'''\n",
    "        zx = self._Zx(X)\n",
    "        ss = 0\n",
    "        for x in X:\n",
    "            if (x, y) in self._numXY:\n",
    "                ss += self._w[self._xyID[(x, y)]]\n",
    "        pyx = math.exp(ss) / zx\n",
    "        return pyx\n",
    "    \n",
    "    def _model_ep(self, index):\n",
    "        '''计算模型期望'''\n",
    "        x, y = self._IDxy[index]\n",
    "        ep = 0\n",
    "        for sample in self._samples:\n",
    "            if x not in sample:\n",
    "                continue\n",
    "            pyx = self._model_pyx(y, sample)\n",
    "            ep += pyx / self._N\n",
    "        return ep\n",
    "    \n",
    "    def _convergence(self): \n",
    "        '''判断是否全部收敛'''\n",
    "        for last, now in zip(self._lastw, self._w):\n",
    "            if abs(last - now) >= self._epsilon:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = self._Zx(X)\n",
    "        result = {}\n",
    "        for y in self._Y:\n",
    "            ss = 0\n",
    "            for x in X:\n",
    "                if (x, y) in self._numXY:\n",
    "                    ss += self._w[self._xyID[(x, y)]]\n",
    "            pyx = math.exp(ss) / Z\n",
    "            result[y] = pyx\n",
    "        return result\n",
    "    \n",
    "    def train(self, maxiter=400):\n",
    "        for loop in range(maxiter):\n",
    "            self._lastw = self._w[:]\n",
    "            for i in range(self._n):\n",
    "                ep = self._model_ep(i)\n",
    "                self._w[i] += math.log(self._Ep_[i] / ep) / self._C\n",
    "            if self._convergence():\n",
    "                break\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型测试**\n",
    "\n",
    "- 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    iris = load_iris();\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    dataset = df.loc[(df['label'] == 0) | (df['label'] == 1),:]\n",
    "    X = dataset.loc[:,'sepal length':'sepal width']\n",
    "    y = dataset['label']\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression_()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-42.04810122, -19.53306557])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大熵模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
    "           ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
    "           ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
    "           ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'mild', 'high', 'TRUE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = MaxEntropy()\n",
    "x = ['overcast', 'mild', 'high', 'FALSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "maxent.fit(dataset)\n",
    "maxent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: {'no': 7.30330395915214e-05, 'yes': 0.9999269669604086}\n"
     ]
    }
   ],
   "source": [
    "print('predict:', maxent.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sckit-learn 实例**\n",
    "\n",
    "sklearn 下有四种优化器，如果不指定默认选用 'lbfgs'，拟牛顿法的一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_lr = LogisticRegression(max_iter=200,solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.73498352 -2.5991523 ]] [-6.76024028]\n"
     ]
    }
   ],
   "source": [
    "print(sk_lr.coef_,sk_lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 习题\n",
    "\n",
    "**6.1 确认逻辑斯蒂分布属于指数分布族**\n",
    "\n",
    "所谓指数分布族(exponential family)，是指如果一类分布属于指数分布族，那么它就可以写成如下形式:\n",
    "$$\n",
    "p(y ; \\eta)=b(y) \\exp \\left(\\eta^{T} T(y)-a(\\eta)\\right)\n",
    "$$\n",
    "$\\eta $ 叫做自然参数（规范参数），$T(y)$ 叫做充分统计量（一般有 $T(y) = y$），$a(\\eta)$ 是对数划分函数，起到正则化常数的作用，保证分布的积分或总和在 y 到 1 之间。\n",
    "\n",
    "选定了 T, a, b 的时候，我们就得到了参数为 $\\eta$ 的分布族，$\\eta$ 取不同的值，可以得到该分布族下不同的分布。\n",
    "\n",
    "可见分布族描述的是输入 $x$ 通过运算得到结果 $y$ ，$y$ 与 $\\eta$ 有某种关系。基于逻辑斯蒂分布得到的基本运算模型是二分类逻辑斯蒂回归模型，$Y=\\{0，1\\}$\n",
    "$$\n",
    "\\begin{array}{l}{P(Y=1 | x)=\\frac{\\exp (w \\cdot x)}{1+\\exp (w \\cdot x)}} \\\\ {P(Y=0 | x)=\\frac{1}{1+\\exp (w \\cdot x)}}\\end{array}\n",
    "$$\n",
    "所以逻辑斯蒂回归模型为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(y|x) &= P(y=1|x)^{y}P(y=0|x)^{1-y}\\\\\n",
    "&= (\\frac{\\exp (w \\cdot x)}{1+\\exp (w \\cdot x)})^y(\\frac{1}{1+\\exp (w \\cdot x)})^{1-y}\\\\\n",
    "&= \\lambda^y(1-\\lambda)^{1-y}\\\\\n",
    "&= \\exp(y\\log \\lambda +(1-y)\\log(1-\\lambda))\\\\\n",
    "&=\\exp(y\\log \\frac{\\lambda}{1-\\lambda}+\\log(1-\\lambda))\n",
    "\\end{aligned}\n",
    "$$\n",
    "令 $T(y)=y,b=1,\\eta = \\log\\frac{\\lambda}{1-\\lambda},a=\\log(1-\\lambda)$，解得 $\\lambda = \\frac{e^\\eta}{1+e^\\eta},a=-\\log(1+e^\\eta)$，符合指数分布族形式，证明完毕\n",
    "\n",
    "**6.2 写出逻辑斯蒂回归模型学习的梯度下降算法**\n",
    "\n",
    "输入：目标函数 $f(w)$,梯度函数 $g(w) = \\nabla f(w)$，计算精度 $\\epsilon$\n",
    "$$\n",
    "f(w)=\\sum_{x} \\tilde{P}(x) \\log \\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)-\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\\\\n",
    "g(w)=\\left(\\frac{\\partial f(w)}{\\partial w_{1}}, \\frac{\\partial f(w)}{\\partial w_{2}}, \\cdots, \\frac{\\partial f(w)}{\\partial w_{n}}\\right)^{\\mathrm{T}}\\\\\n",
    "\\frac{\\partial f(w)}{\\partial w_{i}}=\\sum_{x, y} \\tilde{P}(x) P_{w}(y | x) f_{i}(x, y)-E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\n",
    "$$\n",
    "输出：$f(w)$ 的极小点 $w*$\n",
    "\n",
    "1. 取初始值 $w^{(0)} \\in R^n,k=0$\n",
    "2. 计算 $f(w^{(k)})$\n",
    "3. 计算梯度 $g_k = g(x^{(k)})$，当$\\left\\|g_{k}\\right\\|<\\varepsilon$ ，停止计算，得到 $w^* = w^{(k)}$，否则，令$p_k = -g(w^{(k)})$，求 $\\lambda_k$，使\n",
    "\n",
    "$$\n",
    "f\\left(w^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(w^{(k)}+\\lambda p_{k}\\right)\n",
    "$$\n",
    "\n",
    "4. 置 $w^{(k+1)}=w^{(k)}+\\lambda_{k} p_{k}$，计算 $f(x^{(k+1)})$，当 $\\left\\|f\\left(w^{(k+1)}\\right)-f\\left(w^{(k)}\\right)\\right\\|<\\varepsilon$ 或 $\\left\\|x^{(k+1)}-x^{(k)}\\right\\|<\\varepsilon$ 时，停止迭代，令  $w^*=w^{(k+1)}$\n",
    "5. 否则，置 $k = k+1$,转3\n",
    "\n",
    "**6.3 写出最大熵模型学习的 DFP 算法**\n",
    "\n",
    "输入：目标函数 $f(w)$,梯度函数 $g(w) = \\nabla f(w)$，计算精度 $\\epsilon$\n",
    "\n",
    "输出：$f(w)$ 的极小点 $w*$\n",
    "\n",
    "1. 选定初始点 $w^{(0)}$，取正定对称矩阵 $G_0$，置 $k=0$\n",
    "2. 计算 $g_k = g(w^{(k)})$，如果$\\left\\|g_{k}\\right\\|<\\varepsilon$ ，停止计算，得到 $w^* = w^{(k)}$，否则转3\n",
    "3. 置 $p_k = -G_kg_k$\n",
    "4. 一维搜索，找到 $\\lambda_k$ 使得 $f\\left(w^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(w^{(k)}+\\lambda p_{k}\\right)$\n",
    "5. 置 $w^{(k+1)}=w^{(k)}+\\lambda_{k} p_{k}$\n",
    "6. $g_{k+1} = g(w^{(k+1)})$，如果 $\\left\\|g_{k+1}\\right\\|<\\varepsilon$，停止计算，得 $w^* = w^{(k+1)}$，否则求解 $G_{k+1}$\n",
    "\n",
    "$$\n",
    "G_{k+1}=G_{k}+\\frac{\\delta_{k} \\delta_{k}^{\\mathrm{T}}}{\\delta_{k}^{\\mathrm{T}} y_{k}}-\\frac{G_{k} y_{k} y_{k}^{\\mathrm{T}} G_{k}}{y_{k}^{\\mathrm{T}} G_{k} y_{k}}\n",
    "$$\n",
    "\n",
    "7. 更新 $k = k + 1$，转3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "作者：Daniel Meng\n",
    "\n",
    "GitHub: [LibertyDream](https://github.com/LibertyDream)\n",
    "\n",
    "博客：[明月轩](https://libertydream.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
